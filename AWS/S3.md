# Amazon S3

## Overview
Amazon S3 (Simple Storage Service) is one of the main building blocks of AWS, offering **infinitely scalable storage**. It is widely used as a backbone for many websites and AWS services due to its reliability and flexibility.

### Key Features:
- **Infinitely scalable storage** for objects and files.
- **Globally relied upon** by websites and AWS services for integrations.

---

## Use Cases
1. **Backup and Storage**:
   - Store files, disks, and other data.
2. **Disaster Recovery**:
   - Move data to another region for redundancy in case of regional failures.
3. **Archival**:
   - Archive files for long-term storage at a lower cost.
4. **Hybrid Cloud Storage**:
   - Expand on-premises storage into the cloud.
5. **Hosting Applications and Media**:
   - Host videos, images, and other media files.
6. **Data Lake and Analytics**:
   - Store large datasets for big data analytics.
7. **Delivering Software Updates**:
   - Distribute updates efficiently.
8. **Hosting Static Websites**:
   - Serve static content directly from S3.
9. **Real-World Examples**:
   - **Nasdaq**: Stores 7 years of data in S3 Glacier for archival.
   - **Sysco**: Runs analytics and gains business insights using S3.

---

## Buckets
- **Buckets** are the top-level directories in S3 where objects (files) are stored.
- **Globally Unique Names**:
  - Bucket names must be unique across all AWS accounts and regions.
  - Defined at the **region level**, even though the name is globally unique.
- **Naming Rules**:
  - No uppercase letters or underscores.
  - Length: 3 to 63 characters.
  - Must not resemble an IP address.
  - Must start with a lowercase letter or number.
  - Use letters, numbers, and hyphens for compatibility.

---

## Objects
- **Objects** are the files stored in S3 buckets.
- **Key**:
  - The full path of the object (e.g., `my-folder/another-folder/my-file.txt`).
  - Composed of a **prefix** (e.g., `my-folder/another-folder/`) and an **object name** (e.g., `my-file.txt`).
- **No True Directories**:
  - S3 does not have a concept of directories; everything is stored as keys.
  - The console UI may display directories, but they are just part of the key structure.

---

## Object Details
1. **Content**:
   - The body of the object (e.g., file content).
   - Maximum object size: **5 TB**.
   - Files larger than **5 GB** require **multi-part upload**.
2. **Metadata**:
   - Key-value pairs providing additional information about the object.
   - Can be system-defined or user-defined.
3. **Tags**:
   - Up to 10 Unicode key-value pairs.
   - Useful for security and lifecycle management.
4. **Versioning**:
   - Objects can have a version ID if versioning is enabled.

---

## Summary
Amazon S3 is a versatile and scalable storage solution with a wide range of use cases, from backups and disaster recovery to hosting static websites and performing big data analytics. Its flexibility, combined with features like globally unique buckets, object versioning, and metadata, makes it a cornerstone of AWS infrastructure.


# How to create a S3 bucket 

- Log into your AWS account and search for `S3` 
- Click create Bucket 

![create bucket](/Images/s3.JPG)

- Provide a name which has to be unique and keep the settings to default : 

![create bucket settings](/Images/s31.JPG)
![create bucket settings](/Images/s32.JPG)
![create bucket settings](/Images/s33.JPG)

- lets upload an object : 
    click upload 
    Add files 
    I will just upload a simple .png file, double check the destination and click upload 

- we now have 1 object lets take a closer look 

![create bucket settings](/Images/s34.JPG)

when we click our object we can view different types of information, in the top right corner we can see a `open` button, we can now view what we uploaded. 

- The same concept works for creating folders which hold files like the image we just uploaded 


# S3 security bucket policy 

## Overview
Amazon S3 provides multiple layers of security to control access to buckets and objects. These include user-based permissions, resource-based policies, and encryption.

---

## Security Mechanisms

### 1. **User-Based Security**
- **IAM Policies**:
  - Define which API calls are allowed for specific IAM users or roles.
  - Used to grant permissions to access S3 resources.

### 2. **Resource-Based Security**
- **S3 Bucket Policies**:
  - JSON-based policies applied directly to buckets.
  - Allow or deny access to specific users, accounts, or the public.
  - Common use cases:
    - Grant public access to buckets.
    - Allow cross-account access.
    - Enforce encryption for uploaded objects.
- **Access Control Lists (ACLs)**:
  - Provide finer-grained control at the object or bucket level.
  - Can be disabled and are less commonly used.

### 3. **Encryption**
- Objects can be encrypted using encryption keys to ensure data security.

---

## How IAM and Resource Policies Work Together
- An IAM principal (user or role) can access an S3 object if:
  1. The IAM permissions allow it.
  2. The resource policy (e.g., bucket policy) allows it.
  3. There is no explicit deny in either policy.

---

## Example: S3 Bucket Policy
- **Structure**:
  - **Resource**: Specifies the bucket and objects the policy applies to.
  - **Effect**: Allows or denies actions.
  - **Action**: Specifies the API calls (e.g., `GetObject`).
  - **Principal**: Defines who the policy applies to (e.g., a specific user or `*` for public access).

- **Public Access Example**:
  ```json
  {
    "Version": "2012-10-17",
    "Statement": [
      {
        "Effect": "Allow",
        "Principal": "*",
        "Action": "s3:GetObject",
        "Resource": "arn:aws:s3:::example-bucket/*"
      }
    ]
  }

## Common Scenarios

### 1. **Public Access**
- Attach a bucket policy to allow public access to objects.
- Example: A website visitor accessing files in an S3 bucket.

### 2. **IAM User Access**
- Assign IAM permissions to users within your AWS account to allow access to S3 buckets.

### 3. **EC2 Instance Access**
- Use IAM roles instead of IAM users for EC2 instances to access S3 buckets.
- Attach an instance role with the required permissions.

### 4. **Cross-Account Access**
- Use bucket policies to allow access for IAM users in another AWS account.

---

## Additional Security Settings

### **Block Public Access**
- Prevents buckets from being made public, even if a bucket policy allows it.
- Can be applied at the bucket or account level.
- Recommended for preventing accidental data leaks.

---

## Summary
Amazon S3 security is highly flexible, allowing you to control access at both the user and resource levels. Key mechanisms include IAM policies, bucket policies, ACLs, and encryption. For enhanced security, AWS provides settings like "Block Public Access" to prevent unintended exposure of sensitive data.


# How to make our object public using the url 

- Head back to the bucket you created 
- click the permissions tab 
- edit `block public access` 
    - turn off `block all public access` 

    ```
    
    # remember to only do this if you want to allow full public access 

    ```
    - click save 

![create bucket settings](/Images/s35.JPG)

## Create Bucket policy 

- scroll down and click edit
    - we can check use cases : [S3 bucket policies](https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html?icmpid=docs_amazons3_console)
- click policy generator 
    - settings :
        - S3 bucket policy 
        - effect : allow 
        - principal : * 
        - Actions : getobject 
        - ARN : arn:aws:s3:::hmlr-aaron-test-s3/*
    
    - Add statement 
    - Generate Policy 
    - copy the code to back into your policy section in AWS 
    - save changes 


```json

 {
  "Id": "Policy1742918116340",
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "Stmt1742918101932",
      "Action": [
        "s3:GetObject"
      ],
      "Effect": "Allow",
      "Resource": "arn:aws:s3:::hmlr-aaron-test-s3/*",
      "Principal": "*"
    }
  ]
}

```

- within in your bucket click your object and we should be able to access object url : 

![create bucket settings](/Images/s36.JPG)


 # Static Website Hosting 

- Within your bucket go to the `properties` tab 
- scroll down to static website hosting 
    - click edit, enable, save 
    - Hosting type : checked 
    - index document : `provide a name.html` 
    - save changes 

- within your bucket you now want to upload that .html file 

use as a template and replace the url with the image you used 

```html

<!DOCTYPE html>
<html>
<head>
    <title>My Static Website</title>
</head>
<body>
    <h1>Welcome to My Static Website!</h1>
    <p>This is hosted on Amazon S3.</p>
</body>
<img src="https://hmlr-aaron-test-s3.s3.eu-west-1.amazonaws.com/spartalogo.png" alt="Sparta Logo">
</html>

```

- Go back to properties and click the link you should now see your static website : 


![create bucket settings](/Images/s37.JPG)

 
# Versioning in Amazon S3

## Overview
Versioning in Amazon S3 allows you to manage multiple versions of an object within the same bucket. It is a **bucket-level setting** that must be enabled to use.

---

## Key Features
1. **File Versioning**:
   - When versioning is enabled, every time you upload a file with the same key (name), a new version is created.
   - Versions are tracked as `Version 1`, `Version 2`, and so on.

2. **Protection Against Unintended Deletes**:
   - Deleting a file does not remove it permanently; instead, a **delete marker** is added.
   - You can restore previous versions of the file if needed.

3. **Rollback Capability**:
   - Easily roll back to a previous version of a file, allowing you to recover from accidental changes or deletions.

---

## Best Practices
- **Enable Versioning**:
  - It is recommended to enable versioning on buckets to safeguard against accidental deletions or overwrites.
- **Versioning Notes**:
  - Files uploaded before enabling versioning will have a `null` version.
  - Suspending versioning does not delete existing versions, making it a safe operation.

---

## Summary
Versioning in Amazon S3 is a powerful feature that provides protection against accidental deletions and allows you to roll back to previous versions of files. It is a best practice to enable versioning on buckets to ensure data safety and recovery.


## How to enable 

- Go to properties 
- edit bucket versioning, click enable, save 

- edit your html file 
    - re-upload your file 

- we can now toggle versions : 

![create bucket settings](/Images/s38.JPG)

- if you see null against any files this is because versioning was not enabled 
    - lets delete that new version 
        - select and click delete 
        - follow prompt and click delete object 

if you refresh your website you can now see it reverted to its original state 


# Replication (CRR & SRR)

## Overview
Amazon S3 Replication allows you to replicate objects between S3 buckets. There are two types of replication:
1. **CRR (Cross-Region Replication)**:
   - Replicates objects between buckets in **different regions**.
2. **SRR (Same-Region Replication)**:
   - Replicates objects between buckets in the **same region**.

---

## Key Features
1. **Asynchronous Replication**:
   - Objects are copied in the background between the source and destination buckets.
2. **Versioning Requirement**:
   - Versioning must be enabled on both the source and destination buckets.
3. **Cross-Account Replication**:
   - Buckets can belong to different AWS accounts.
4. **IAM Permissions**:
   - Proper IAM permissions must be granted to the S3 service to read from the source bucket and write to the destination bucket.

---

## Use Cases

### **CRR (Cross-Region Replication)**:
- **Compliance**:
  - Meet regulatory requirements by storing data in multiple regions.
- **Lower Latency**:
  - Provide faster access to data by replicating it closer to users in another region.
- **Cross-Account Replication**:
  - Share data securely across AWS accounts.

### **SRR (Same-Region Replication)**:
- **Log Aggregation**:
  - Aggregate logs from multiple buckets into a single bucket for analysis.
- **Production and Test Environments**:
  - Perform live replication between production and test accounts to maintain a synchronized test environment.

---

## Summary
Amazon S3 Replication (CRR and SRR) is a powerful feature for replicating data between buckets, either across regions or within the same region. It supports use cases like compliance, latency optimization, log aggregation, and maintaining synchronized environments. To enable replication, ensure versioning is turned on for both buckets and grant the necessary IAM permissions to the S3 service.

# Replication Example 

- we need two buckets so lets create one more 
    - on the target bucket enable versioning 

- on the origin bucket lets create our replication rule 
    - click the management tab and click create replication rule
    - copy the details and click save  

![create bucket settings](/Images/s39.JPG)
![create bucket settings](/Images/s310.JPG)

When you provided a prompt in this example say no 

upload a file in your original bucket and then head over to your second bucket and if you re fresh it should transfer over 



# Amazon S3 Storage Classes

## Overview
Amazon S3 offers multiple storage classes to optimize cost and performance based on data access patterns. These classes are designed for different use cases, ranging from frequently accessed data to long-term archival storage.

---

## Key Storage Classes

1. **S3 Standard (General Purpose)**:
   - **Use Case**: Frequently accessed data.
   - **Availability**: 99.99%.
   - **Features**: Low latency, high throughput, and can sustain two concurrent facility failures.
   - **Examples**: Big data analytics, mobile and gaming applications, content distribution.

2. **S3 Standard-IA (Infrequent Access)**:
   - **Use Case**: Data accessed less frequently but requires rapid access when needed.
   - **Availability**: 99.9%.
   - **Features**: Lower cost than S3 Standard but includes retrieval costs.
   - **Examples**: Disaster recovery, backups.

3. **S3 One Zone-IA**:
   - **Use Case**: Data that can be recreated or is non-critical.
   - **Availability**: 99.5%.
   - **Features**: Stored in a single availability zone, lower cost but less durable.
   - **Examples**: Secondary backups, on-premises data copies.

4. **S3 Glacier Instant Retrieval**:
   - **Use Case**: Archival data that needs millisecond retrieval.
   - **Features**: Low-cost storage with a minimum storage duration of 90 days.
   - **Examples**: Data accessed once a quarter.

5. **S3 Glacier Flexible Retrieval**:
   - **Use Case**: Archival data with flexible retrieval times.
   - **Features**:
     - **Expedited**: 1–5 minutes.
     - **Standard**: 3–5 hours.
     - **Bulk**: 5–12 hours (free).
   - **Examples**: Long-term backups, compliance data.

6. **S3 Glacier Deep Archive**:
   - **Use Case**: Long-term archival data with infrequent access.
   - **Features**:
     - **Standard Retrieval**: 12 hours.
     - **Bulk Retrieval**: 48 hours.
     - Minimum storage duration: 180 days.
   - **Examples**: Historical records, regulatory archives.

7. **S3 Intelligent-Tiering**:
   - **Use Case**: Automatically moves objects between tiers based on access patterns.
   - **Features**:
     - Frequent Access Tier (default).
     - Infrequent Access Tier (after 30 days of no access).
     - Archive Instant Access Tier (after 90 days of no access).
     - Archive Access and Deep Archive Access Tiers (optional, configurable for 90–700+ days).
   - **Examples**: Data with unpredictable access patterns.

---

## Key Concepts

1. **Durability**:
   - All storage classes have **11 nines durability** (99.999999999%).
   - This means that if you store 10 million objects, you might lose one object every 10,000 years.

2. **Availability**:
   - Varies by storage class:
     - S3 Standard: 99.99%.
     - S3 Standard-IA: 99.9%.
     - S3 One Zone-IA: 99.5%.

3. **Lifecycle Management**:
   - Use S3 Lifecycle configurations to automatically transition objects between storage classes based on access patterns.

---

## Summary of Storage Classes
- **S3 Standard**: Frequently accessed data.
- **S3 Standard-IA**: Infrequently accessed data with rapid retrieval.
- **S3 One Zone-IA**: Infrequently accessed data stored in a single AZ.
- **S3 Glacier Instant Retrieval**: Archival data with millisecond retrieval.
- **S3 Glacier Flexible Retrieval**: Archival data with flexible retrieval times.
- **S3 Glacier Deep Archive**: Long-term archival data with the lowest cost.
- **S3 Intelligent-Tiering**: Automatically optimizes storage costs based on access patterns.

---

## Explanation of Attached Tables

### **Storage Classes Comparison**
- **Durability**: All classes have 11 nines durability.
- **Availability**: Higher availability for frequently accessed data (e.g., S3 Standard) and lower for archival classes.
- **Minimum Storage Duration**: Some classes (e.g., Glacier) require a minimum storage duration to optimize costs.

![create bucket settings](/Images/s312.JPG)

### **Pricing Comparison**
- **Storage Costs**: Vary by class, with S3 Standard being the most expensive and Glacier Deep Archive the least.
- **Retrieval Costs**: Archival classes (e.g., Glacier) include retrieval fees, while S3 Standard and Intelligent-Tiering do not.
- **Retrieval Times**: Instant for S3 Standard and Intelligent-Tiering, while Glacier classes have varying retrieval times.

![create bucket settings](/Images/s311.JPG)


---

## Notes
- You don't need to memorize the exact numbers from the tables.
- Focus on understanding the use cases and differences between storage classes.


# creating our storage class 

- create a new bucket and upload a file 
- if you scroll down and click `properties` you will see storage classes 
- pick one and click upload at the bottom, you can edit this again 

## create a lifecycle rule 

- go back to your bucket anc click management 
- click create `life cycle rule` 
    - provide a name 
    - apply to all objects in the bucket 
        acknowledge prompt 
    - move current versions of objects between storage classes 
    - your first storage class = x amount of days 
        - click add transition, pick your next one and so on  
    - create rule 


# S3 Encryption

## Overview
Amazon S3 provides encryption to secure your data, and there are two main types of encryption: **Server-Side Encryption** and **Client-Side Encryption**.

---

## Types of Encryption

### 1. **Server-Side Encryption (SSE)**
- **Default Behavior**:
  - By default, whenever you create a bucket or upload an object, server-side encryption is enabled.
- **How It Works**:
  - The user uploads an object to S3.
  - Once the object arrives in the bucket, Amazon S3 encrypts it automatically for security purposes.
- **Key Point**:
  - The encryption is handled by the server, hence the name **Server-Side Encryption**.

### 2. **Client-Side Encryption**
- **How It Works**:
  - The user encrypts the file **before uploading it** to S3.
  - The encrypted file is then uploaded to the bucket.
- **Key Point**:
  - The encryption is handled by the user, not the server, hence the name **Client-Side Encryption**.

---

## Summary
- **Server-Side Encryption** is the default and ensures that objects are encrypted automatically when they are uploaded to S3.
- **Client-Side Encryption** requires the user to encrypt the data before uploading it to S3.
- Both models exist in AWS, but **Server-Side Encryption** is always on by default.


# IAM Access Analyzer for Amazon S3

## Overview
IAM Access Analyzer for Amazon S3 is a **monitoring feature** that helps ensure only the intended people or entities have access to your S3 buckets.

---

## Key Features
1. **Analyzes Access Policies**:
   - Reviews **Bucket Policies**, **S3 ACLs**, and **S3 Access Point Policies**.
   - Identifies buckets that are publicly accessible or shared with other AWS accounts.

2. **Highlights Potential Security Issues**:
   - Surfaces buckets that may have unintended access.
   - Allows you to review and determine if the access is expected or a security concern.

3. **Powered by IAM Access Analyzer**:
   - Detects resources in your account that are shared with external entities.
   - Helps you take corrective actions to secure your buckets.

---

## Use Case
- **Security Monitoring**:
  - Identify and address unintended public access or cross-account sharing of S3 buckets.

---

## Summary
IAM Access Analyzer for Amazon S3 is a valuable tool for monitoring and securing your S3 buckets. It ensures that only authorized entities have access by analyzing policies and highlighting potential security issues.


# Shared Responsibility Model for Amazon S3

## Overview
The shared responsibility model for Amazon S3 defines the responsibilities of AWS and the user. AWS manages the infrastructure, while the user is responsible for securing and configuring their S3 buckets.

---

## AWS Responsibilities
1. **Infrastructure Management**:
   - Ensures durability and availability of S3.
   - Can sustain the loss of two facilities.
2. **Internal Security**:
   - Handles internal configuration and vulnerability analysis.
3. **Compliance Validation**:
   - Ensures compliance within AWS infrastructure.

---

## User Responsibilities
1. **Data Protection**:
   - Set up **S3 Versioning** to protect data.
   - Configure the correct **S3 Bucket Policies** to secure access.
2. **Encryption**:
   - Enable encryption for data stored in S3 buckets.
3. **Logging and Monitoring**:
   - Enable logging and monitoring for better visibility.
4. **Cost Optimization**:
   - Choose the most cost-effective storage class for your data.
5. **Verification**:
   - Set up verification mechanisms if required.

---

## Summary
AWS is responsible for managing the infrastructure and ensuring the durability and availability of S3. Users are responsible for securing their data, configuring bucket policies, enabling encryption, and optimizing costs. This division of responsibilities ensures a secure and efficient use of Amazon S3.


# AWS Snowball

## Overview
AWS Snowball is a **highly secure and portable device** used for:
1. **Data Migration**: Transferring large amounts of data (e.g., petabytes) in and out of AWS.
2. **Edge Computing**: Collecting and processing data at edge locations with limited or no internet connectivity.

---

## Types of Snowball Devices
1. **Edge Storage Optimized**:
   - **Storage Capacity**: 210 terabytes.
   - **Use Case**: Large-scale data migrations.

2. **Edge Compute Optimized**:
   - **Storage Capacity**: 28 terabytes.
   - **Use Case**: Edge computing with compute power for running EC2 instances or Lambda functions.

---

## Use Cases

### 1. **Data Migration**
- **Challenges with Network Transfers**:
  - Slow connections or limited bandwidth.
  - High network costs.
  - Unstable connections or shared bandwidth.
- **Solution**:
  - Use Snowball to physically transfer data.
  - Process:
    1. AWS ships the Snowball device to your location.
    2. You load the device with your data.
    3. Ship the device back to AWS.
    4. AWS imports the data into services like Amazon S3.

### 2. **Edge Computing**
- **Use Case**:
  - Process data at edge locations (e.g., trucks, ships, mining stations) with limited or no internet access.
- **Capabilities**:
  - Run EC2 instances or Lambda functions directly on the device.
  - Perform tasks like:
    - Pre-processing data.
    - Machine learning at the edge.
    - Media transcoding at the edge.
- **Workflow**:
  - Process data locally on the Snowball device.
  - Send the processed data back to AWS when connectivity is available.

---

## Summary
AWS Snowball is ideal for:
1. **Data Migration**: Efficiently transferring large datasets without relying on slow or costly network connections.
2. **Edge Computing**: Processing data locally in remote or disconnected environments with Compute Optimized devices.

Snowball simplifies data transfer and enables advanced edge computing use cases, making it a versatile solution for both migration and edge processing needs.


# Order AWS snow family device 

- search snow family on AWS 
- click order 

    - page 1 : 
        - provide a name 
        - import into amazon s3 


![create bucket settings](/Images/s313.JPG)


    - page 2 :
        -  click : snowball edge storage optimized with 210TB
        -  pricing plan : on demand, per day pricing 
        -  storage type : s3 data transfer 
        -  select your s3 bucket  

    
    - page 3 : 
        - no features and options 


    - page 4 : 
        - encryption is default 
        - create service role 
        - shipping address 
        - shipping speed 
        - shipping notifications 

    - page 5 :
        - job summary 
        - place order 


# Snowball Edge Pricing

## Overview
Snowball Edge pricing includes charges for device usage and data transfer. Here's a breakdown of the costs:

---

## Key Pricing Details

1. **Data Transfer Costs**:
   - **Data into Amazon S3**: Free ($0 per gigabyte).
   - **Data out of AWS**: Charged based on the amount of data transferred.

2. **Device Usage Costs**:
   - **On-Demand Pricing**:
     - One-time service fee per job.
     - Includes:
       - **10 days of usage** for the 80 TB Snowball Edge Storage Optimized.
       - **15 days of usage** for the 210 TB Snowball Edge Storage Optimized.
     - **Shipping Days**: Not counted towards the usage limit (shipping is free).
     - Additional usage beyond the included days is charged per day.
   - **Committed Upfront Pricing**:
     - Pay in advance for monthly, 1-year, or 3-year usage.
     - Designed for edge computing use cases.
     - Offers up to **62% discounted pricing** for long-term commitments.

---

## Summary
- **Free**: Data transferred into Amazon S3.
- **Charged**: Data transferred out of AWS and device usage beyond the included days.
- **On-Demand**: Pay per job with included usage days.
- **Committed Upfront**: Discounted pricing for long-term usage.

From an exam perspective, remember that **data into Amazon S3 is free**, while other operations incur costs.


# Amazon S3 in a Hybrid Cloud Setting

## Overview
Amazon S3 can be used in a **hybrid cloud** setup, bridging your **on-premises infrastructure** with AWS. This allows you to leverage both on-premises and cloud storage for various use cases.

---

## Why Use a Hybrid Cloud?
1. **Migration**:
   - Gradual migration from on-premises to the cloud.
2. **Compliance and Security**:
   - Meet specific regulatory or security requirements by keeping part of your infrastructure on-premises.
3. **Strategy**:
   - Some organizations prefer a mix of on-premises and cloud environments for flexibility.

---

## Storage Gateway
To expose Amazon S3 data on-premises, AWS provides the **Storage Gateway**, which acts as a bridge between on-premises systems and AWS cloud storage.

### **Use Cases**:
- **Disaster Recovery**:
  - Seamlessly back up on-premises data to AWS.
- **Backup and Restore**:
  - Store backups in the cloud for long-term retention.
- **Tiered Storage**:
  - Extend on-premises storage capacity by offloading data to AWS.

### **Types of Storage Gateway**:
1. **File Gateway**:
   - Provides file-based access to S3.
2. **Volume Gateway**:
   - Provides block storage for on-premises applications.
3. **Tape Gateway**:
   - Virtual tape library for backup applications.

---

## Summary
- **Hybrid Cloud**: Combines on-premises and cloud storage for flexibility, compliance, and disaster recovery.
- **Storage Gateway**: Bridges on-premises systems with AWS, enabling seamless integration with services like Amazon S3, EBS, and Glacier.
- **Key Use Cases**: Disaster recovery, backup and restore, and tiered storage.

For the Certified Cloud Practitioner exam, focus on understanding that the **Storage Gateway** enables hybrid storage by connecting on-premises systems to AWS.


# Summary 

![create bucket settings](/Images/s314.JPG)


# Questions : 

- which s3 storage class is the most cost effective for archiving data with no retrieval time requirement ? 

- what hybrid aws service is used to allow on prem servers to access the aws cloud at the storage layer ? 

- what are objects not compose of ? 

- where are objects stored in amazon s3 ? 

- what can you use to define actions to move s3 objects between different storage classes ? 

- A non-profit organization needs to regularly transfer petabytes of data to the cloud and to have access to local computing capacity. Which service can help with this task ? 

- Which S3 Storage Class is suitable for less frequently accessed data, but with rapid access when needed, while keeping a high durability and allowing an Availability Zone failure ?





